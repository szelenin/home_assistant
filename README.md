# Home Assistant

A Python-based voice-controlled home automation system with speech recognition and text-to-speech capabilities.

## Features

- **Voice Control**: Speech recognition for hands-free operation
- **Multi-Provider Text-to-Speech**: Support for pyttsx3, eSpeak-NG, and Piper neural TTS with configurable providers
- **Wake Word Detection**: Customizable wake word for activation
- **AI Integration**: Claude (Anthropic) and ChatGPT (OpenAI) support with automatic fallback
- **Intent Recognition**: Understands weather, device control, personal info, and general questions
- **Natural Language Processing**: Translates commands to device API calls
- **Device Management**: Control smart lights, thermostats, and other IoT devices
- **Automation**: Create custom automation rules for your smart home
- **Configuration**: YAML-based configuration system
- **Multi-Engine Speech Recognition**: Support for Google, Vosk, Sphinx, and Whisper

## Getting Started

### Prerequisites

- Python 3.8 or higher
- macOS (for voice features) or Linux
- Git

### System Dependencies

#### Core Dependencies (Required)

**PortAudio** - Required for speech recognition (PyAudio)

**macOS:**
```bash
brew install portaudio
```

**Ubuntu/Debian:**
```bash
sudo apt-get install portaudio19-dev python3-pyaudio
```

**Fedora/RHEL/CentOS:**
```bash
sudo dnf install portaudio-devel
```

**Arch Linux:**
```bash
sudo pacman -S portaudio
```

#### TTS Engine Dependencies (Optional)

Choose the TTS engines you want to use. The default `piper` provider offers the highest quality neural speech synthesis.

##### eSpeak-NG (Direct Provider)
For using the direct eSpeak TTS provider:

**macOS:**
```bash
brew install espeak-ng
```

**Ubuntu/Debian:**
```bash
sudo apt-get install espeak-ng
```

**Fedora/RHEL/CentOS:**
```bash
sudo dnf install espeak-ng
```

**Arch Linux:**
```bash
sudo pacman -S espeak-ng
```

##### Piper Neural TTS (Default - Model Download Required)
Piper TTS is the default high-quality neural TTS engine. It requires downloading neural voice models:

**1. Download Voice Models:**
Choose and download models from [Piper Releases](https://github.com/rhasspy/piper/releases):

**Popular English Models:**
```bash
# Download en_US-lessac-medium (Female US English) - Recommended
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/en_US-lessac-medium.onnx
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/en_US-lessac-medium.onnx.json

# Download en_US-danny-low (Male US English)
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/en_US-danny-low.onnx
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/en_US-danny-low.onnx.json

# Download en_GB-alan-medium (Male UK English)
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/en_GB-alan-medium.onnx
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/en_GB-alan-medium.onnx.json
```

**Other Languages:**
```bash
# German
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/de_DE-thorsten-medium.onnx
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/de_DE-thorsten-medium.onnx.json

# French
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/fr_FR-mls_1840-low.onnx
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/fr_FR-mls_1840-low.onnx.json

# Spanish
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/es_ES-mls_9972-low.onnx
wget https://github.com/rhasspy/piper/releases/download/v1.2.0/es_ES-mls_9972-low.onnx.json
```

**2. Place model files in your project directory:**
```bash
# Models should be in the same directory as your main.py
ls *.onnx *.onnx.json
# en_US-lessac-medium.onnx  en_US-lessac-medium.onnx.json
```

**3. Update config.yaml:**
```yaml
tts:
  provider: piper
  providers:
    piper:
      model: en_US-lessac-medium  # Model name without .onnx extension
```

**Model Size Reference:**
- **X-Low quality**: ~20MB, basic quality, fast
- **Low quality**: ~40MB, good quality, fast  
- **Medium quality**: ~60MB, high quality, moderate speed
- **High quality**: ~100MB+, excellent quality, slower

**Browse all available models**: [Piper Voice Samples](https://rhasspy.github.io/piper-samples/)

### Installation

1. Clone the repository:
   ```bash
   git clone git@github.com:szelenin/home_assistant.git
   cd home_assistant
   ```

2. Create and activate virtual environment:
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. Install Python dependencies:
   ```bash
   # Install core dependencies (includes Piper TTS by default)
   pip install -r requirements.txt
   
   # Alternative: Install specific TTS engine only
   pip install -r requirements-tts-piper.txt    # Neural TTS (default, best quality)
   pip install -r requirements-tts-pyttsx.txt   # System TTS (lightweight)
   pip install -r requirements-tts-espeak.txt   # Direct eSpeak (minimal)
   
   # Full installation (all speech recognition engines)
   pip install -r requirements-full.txt
   ```

4. Configure AI Provider (Required):
   
   The system uses AI providers for natural language understanding. You need at least one API key:

   **Step 1: Get API Key**
   
   **Option A: Anthropic Claude (Recommended)**
   - Sign up at [Anthropic Console](https://console.anthropic.com/)
   - Go to [API Keys](https://console.anthropic.com/settings/keys) and create a new key
   - Add credits to your account at [Billing](https://console.anthropic.com/settings/billing)
   
   **Option B: OpenAI ChatGPT**
   - Sign up at [OpenAI Platform](https://platform.openai.com/)
   - Go to [API Keys](https://platform.openai.com/api-keys) and create a new key
   - Add billing information at [Billing](https://platform.openai.com/settings/organization/billing/overview)

   **Step 2: Create AI Configuration**
   ```bash
   # Copy the template file
   cp ai_config.example.yaml ai_config.yaml
   
   # Edit with your API keys
   nano ai_config.yaml
   ```

   **Step 3: Add your API keys to ai_config.yaml:**
   ```yaml
   # API Keys (replace with your actual keys)
   anthropic_api_key: "your-anthropic-key-here"  # For Claude
   openai_api_key: "your-openai-key-here"        # For ChatGPT
   ```
   
   **Note**: `ai_config.yaml` is automatically ignored by git to keep your API keys secure.

5. Configure other settings:
   ```bash
   # Edit config.yaml to customize voice and speech settings
   nano config.yaml
   ```

6. Run the application:
   ```bash
   python main.py
   ```

## Configuration

### Voice Settings (`config.yaml`)

The system now supports multiple TTS providers with configurable settings:

```yaml
tts:
  provider: piper  # Options: piper, pyttsx, espeak (piper is default)
  providers:
    piper:
      model: en_US-lessac-medium
      rate: 1.0
      volume: 1.0
      speaker_id: null
      output_raw: false
    pyttsx:
      voice_id: "com.apple.voice.compact.en-US.Samantha"
      rate: 150
      volume: 0.5
    espeak:
      voice: en
      rate: 175
      volume: 80
      pitch: 50
      gap: 0

speech:
  language: en-US  # Speech recognition language
  recognition_engines:
    - google  # Primary (requires internet)
    - vosk    # Secondary (offline fallback)
    - sphinx  # Tertiary (offline fallback)

wake_word:
  name: null  # Custom wake word (null for default)
  sensitivity: 0.5  # Wake word sensitivity
```

### Speech Recognition Engines

The system supports multiple speech recognition engines with automatic fallback:

**Online Engines:**
- **Google Speech Recognition** (default) - High accuracy, requires internet
- **Google Cloud Speech** - Enterprise-grade, requires API key
- **Azure Speech Services** - Microsoft's speech recognition
- **Amazon Transcribe** - AWS speech recognition

**Offline Engines:**
- **Vosk** - Fast, accurate offline recognition
- **Sphinx** - CMU's offline speech recognition
- **Whisper** - OpenAI's offline speech recognition

**Engine Priority:**
1. **Google** (online, best accuracy)
2. **Vosk** (offline, good accuracy)
3. **Sphinx** (offline, basic accuracy)

### Text-to-Speech Providers

The system supports multiple TTS providers with automatic availability detection and graceful fallback:

#### 1. Piper Neural TTS (Default)
**Description:** High-quality neural text-to-speech  
**Status:** ‚úÖ Included in requirements.txt, requires model download  
**Quality:** Excellent, human-like  
**Requirements:** Neural voice models (see System Dependencies section above)

**Installation:**
```bash
# Install with specific requirements file
pip install -r requirements-tts-piper.txt
```

**Popular Models:**
- `en_US-lessac-medium` - Female US English (recommended, included)
- `en_US-ljspeech-medium` - Female US English
- `en_US-danny-low` - Male US English  
- `en_GB-alan-medium` - Male UK English
- `de_DE-thorsten-medium` - German
- `fr_FR-mls_1840-low` - French

**Configuration:**
```yaml
tts:
  provider: piper
  providers:
    piper:
      model: en_US-lessac-medium  # Model name (without .onnx extension)
      rate: 1.0                   # Speed multiplier (0.25-4.0)
      volume: 1.0                 # Volume multiplier (0.0-2.0)
      speaker_id: null            # Speaker ID for multi-speaker models
      output_raw: false           # Stream raw audio
```

#### 2. Pyttsx (System TTS)
**Description:** Uses pyttsx3 library with system TTS engines  
**Status:** ‚úÖ Available on all platforms, no setup required  
**Quality:** Good, uses system voices  
**Requirements:** No additional installation

**Installation:**
```bash
# Install with specific requirements file
pip install -r requirements-tts-pyttsx.txt
```

**Configuration:**
```yaml
tts:
  provider: pyttsx
  providers:
    pyttsx:
      voice_id: "com.apple.voice.compact.en-US.Samantha"  # System voice ID
      rate: 150      # Words per minute (50-400)
      volume: 0.5    # Volume level (0.0-1.0)
```

#### 3. eSpeak-NG (Direct)
**Description:** Direct subprocess calls to eSpeak-NG  
**Status:** ‚ö†Ô∏è Requires system installation  
**Quality:** Basic, robotic but clear  
**Requirements:** Install eSpeak-NG system package (see System Dependencies section above)

**Installation:**
```bash
# Install with specific requirements file  
pip install -r requirements-tts-espeak.txt

# System installation required (see System Dependencies section)
```

**Configuration:**
```yaml
tts:
  provider: espeak
  providers:
    espeak:
      voice: en        # Language/voice code
      rate: 175        # Words per minute (80-450)
      volume: 80       # Volume level (0-200, 100=normal)
      pitch: 50        # Pitch level (0-99)
      gap: 0           # Gap between words (10ms units)
```

**Available eSpeak voices:** Run `espeak-ng --voices` to see all available voices

#### Provider Selection Priority
1. **Piper Neural TTS** (default, highest quality)
2. **Automatic fallback** to pyttsx if Piper fails or models unavailable
3. **Availability detection** - unavailable providers are automatically skipped

#### Testing TTS Providers
```bash
# Test all available providers
python tests/integration/test_tts.py

# Check provider availability
python -c "
from home_assistant.speech.tts import TextToSpeech
tts = TextToSpeech()
print('Available providers:', tts.get_available_providers())
"
```

### Available Voices

#### Pyttsx Voices (System Voices)
**macOS:**
- `com.apple.voice.compact.en-US.Samantha` (Female, US English)
- `com.apple.voice.compact.en-US.Alex` (Male, US English)
- `com.apple.voice.compact.en-GB.Daniel` (Male, UK English)

**Linux/Windows:**
Voice availability depends on system installation. Common voices include:
- English voices (various)
- Multi-language support based on system TTS

**List available voices:**
```python
from home_assistant.speech.tts import TextToSpeech
tts = TextToSpeech('pyttsx')
tts.list_voices()
```

#### eSpeak Voices
eSpeak supports 100+ languages and variants:
- `en` - English (default)
- `en-us` - US English
- `en-gb` - British English
- `de` - German
- `fr` - French
- `es` - Spanish
- `it` - Italian
- And many more...

**List available eSpeak voices:**
```bash
espeak-ng --voices
```

#### Piper Neural Voices
High-quality neural voices (requires model download):
- **English:** lessac, ljspeech, amy, danny, ryan, and more
- **Multi-language:** German, French, Spanish, Italian, Dutch, etc.
- **Voice samples:** Available on [Piper releases page](https://github.com/rhasspy/piper/releases)

## Testing

### Scenario-Based Testing

The project uses **scenario-based testing** for better organization and clarity. Each scenario tests specific functionality in a standardized way.

#### Run All Scenarios
```bash
python tests/run_scenarios.py
```

#### Run Specific Scenario Categories
```bash
# Test TTS functionality (legacy scenarios)
python tests/run_scenarios.py --scenario tts

# Test multi-provider TTS system (new)
python tests/integration/test_tts.py

# Test speech recognition
python tests/run_scenarios.py --scenario recognizer

# Test integration between TTS and recognizer
python tests/run_scenarios.py --scenario integration

# Test name collection functionality
python tests/run_scenarios.py --scenario name_collection
```

#### Individual Scenario Files
```bash
# Run individual scenario files directly
python tests/scenarios/tts_scenarios.py
python tests/scenarios/recognizer_scenarios.py
python tests/scenarios/integration_scenarios.py
python tests/scenarios/name_collection_scenarios.py
```

### Legacy Tests (Still Available)
```bash
# Original test files (for backward compatibility)
python tests/test_recognizer.py
python tests/test_tts.py
python tests/test_recognizer_tts_integration.py
python tests/test_config.py
```

### Test Structure

```
tests/
‚îú‚îÄ‚îÄ run_scenarios.py                    # Main test runner
‚îú‚îÄ‚îÄ scenarios/                          # Scenario-based tests
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ tts_scenarios.py               # TTS test scenarios
‚îÇ   ‚îú‚îÄ‚îÄ recognizer_scenarios.py        # Speech recognition scenarios
‚îÇ   ‚îú‚îÄ‚îÄ integration_scenarios.py       # TTS + Recognizer integration
‚îÇ   ‚îî‚îÄ‚îÄ name_collection_scenarios.py   # Name collection scenarios
‚îú‚îÄ‚îÄ test_recognizer.py                 # Legacy recognizer tests
‚îú‚îÄ‚îÄ test_tts.py                        # Legacy TTS tests
‚îú‚îÄ‚îÄ test_recognizer_tts_integration.py # Legacy integration tests
‚îî‚îÄ‚îÄ test_config.py                     # Configuration tests
```

### Scenario Categories

**üé§ TTS Scenarios:**
- Multi-provider testing (pyttsx, espeak, piper)
- Provider availability detection
- Voice configuration per provider
- Error handling and fallback
- Legacy: Welcome message testing, short phrases, long text handling

**üéµ Recognizer Scenarios:**
- Microphone initialization
- Ambient noise adjustment
- Single speech recognition
- Continuous recognition
- Engine fallback testing

**üé§üéµ Integration Scenarios:**
- Speak and listen (TTS ‚Üí Recognizer)
- Conversation flow simulation
- Configuration testing
- Error handling

**üìù Name Collection Scenarios:**
- Initial setup testing
- Name collection flow
- Configuration management
- Error handling

## Project Structure

```
home_assistant/
‚îú‚îÄ‚îÄ config.yaml              # Main configuration file
‚îú‚îÄ‚îÄ main.py                  # Application entry point
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îú‚îÄ‚îÄ requirements-full.txt    # All dependencies (including optional)
‚îú‚îÄ‚îÄ setup.py                # Package installation script
‚îú‚îÄ‚îÄ home_assistant/         # Main package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py            # Application entry point
‚îÇ   ‚îú‚îÄ‚îÄ speech/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recognizer.py  # Speech recognition
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tts.py        # Text-to-speech
‚îÇ   ‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py     # Configuration management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py     # Logging utilities
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ name_collector.py
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ chatgpt.py        # AI integration
‚îÇ   ‚îî‚îÄ‚îÄ chatgpt_showcase.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ run_scenarios.py   # Main test runner
‚îÇ   ‚îú‚îÄ‚îÄ scenarios/         # Scenario-based tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tts_scenarios.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recognizer_scenarios.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ integration_scenarios.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ name_collection_scenarios.py
‚îÇ   ‚îú‚îÄ‚îÄ test_recognizer.py # Legacy tests
‚îÇ   ‚îú‚îÄ‚îÄ test_tts.py
‚îÇ   ‚îú‚îÄ‚îÄ test_recognizer_tts_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_config.py
‚îú‚îÄ‚îÄ logs/                  # Application logs
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ high level design.md
```

## Troubleshooting

### Speech Recognition Issues
- **"Could not find PyAudio"**: Install PortAudio first, then PyAudio
- **Microphone not working**: Check system permissions and microphone settings
- **Speech not recognized**: Ensure good microphone quality and clear speech

### Text-to-Speech Issues

#### General TTS Issues
- **No sound**: Check audio device settings and volume
- **Provider not available**: Check system dependencies and installation
- **Fallback to pyttsx**: Other providers failed, check logs for details

#### Pyttsx Issues
- **Wrong voice**: Update `voice_id` in `providers.pyttsx` section
- **Rate issues**: Adjust `rate` setting (50-400 WPM)
- **macOS voice issues**: Some voices may not respect rate/volume changes

#### eSpeak Issues
- **"espeak-ng not found"**: Install eSpeak-NG system package
  ```bash
  # macOS: brew install espeak-ng
  # Ubuntu: sudo apt-get install espeak-ng
  ```
- **Robotic voice**: Normal for eSpeak, try adjusting pitch and rate
- **Voice not found**: Run `espeak-ng --voices` to see available voices

#### Piper Issues
- **"Unable to find voice" error**: Download the required model files
  1. Visit [Piper Releases](https://github.com/rhasspy/piper/releases)
  2. Download both `.onnx` and `.onnx.json` files
  3. Place in project directory
- **Model loading slow**: Large models take time to load initially
- **Audio quality issues**: Try different models or adjust volume settings

#### Debugging TTS Issues
```bash
# Check provider availability
python -c "
from home_assistant.speech.tts import TextToSpeech
tts = TextToSpeech()
print('Available providers:', tts.get_available_providers())
for name, available in tts.get_available_providers().items():
    if available:
        provider_tts = TextToSpeech(name)
        print(f'{name} info:', provider_tts.get_provider_info())
"

# Test specific provider
python -c "
from home_assistant.speech.tts import TextToSpeech
tts = TextToSpeech('espeak')  # or 'piper', 'pyttsx'
print('Testing provider:', tts.provider_name)
success = tts.speak('Hello, this is a test.')
print('Success:', success)
"
```

## Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Contact

- GitHub: [@szelenin](https://github.com/szelenin)
- Project Link: [https://github.com/szelenin/home_assistant](https://github.com/szelenin/home_assistant) 